{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance Autocomplete RL Training\n",
    "\n",
    "This notebook trains a small language model to perform financial data autocomplete using reinforcement learning with tool calls.\n",
    "\n",
    "The model learns to:\n",
    "1. Use financial tools (get_metrics, get_tickers, get_value, calculate) to retrieve data\n",
    "2. Complete text with accurate financial information\n",
    "3. Determine when no completion is needed\n",
    "\n",
    "Training uses the ART (Adaptive Reinforcement Training) framework with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install openpipe-art==0.3.11.post3 \"gql<4\" --prerelease allow --no-cache-dir\n",
    "!pip install openpipe aiosqlite httpx tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Required: OpenAI API key for LLM-as-judge evaluation\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "else:\n",
    "    print(\"Warning: No OpenAI API key provided. Judge evaluation will fall back to simple string matching.\")\n",
    "\n",
    "# Required: Tiingo API for financial data\n",
    "TIINGO_API_KEY = os.getenv(\"TIINGO_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if TIINGO_API_KEY:\n",
    "    os.environ[\"TIINGO_API_KEY\"] = TIINGO_API_KEY\n",
    "else:\n",
    "    print(\"ERROR: TIINGO_API_KEY is required. Get a free key at https://api.tiingo.com\")\n",
    "\n",
    "# Optional: Weights & Biases for metrics tracking\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "# Optional: OpenPipe for completion logging\n",
    "OPENPIPE_API_KEY = os.getenv(\"OPENPIPE_API_KEY\", \"\")  # @param {type:\"string\"}\n",
    "if OPENPIPE_API_KEY:\n",
    "    os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install openpipe-art==0.3.11.post3 \"gql<4\" --prerelease allow --no-cache-dir\n",
    "!pip install aiosqlite httpx tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Required: OpenAI API key for LLM-as-judge evaluation\n",
    "OPENAI_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "else:\n",
    "    print(\"Warning: No OpenAI API key provided. Judge evaluation will fall back to simple string matching.\")\n",
    "\n",
    "# Optional: Weights & Biases for metrics tracking\n",
    "WANDB_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "# Optional: Tiingo API for real financial data\n",
    "TIINGO_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "if TIINGO_API_KEY:\n",
    "    os.environ[\"TIINGO_API_KEY\"] = TIINGO_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Finance Modules\n",
    "\n",
    "We'll embed the core modules directly in the notebook for Colab compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_STEPS = 100  # @param {type:\"integer\"}\n",
    "NUM_CASES_PER_STEP = 10  # @param {type:\"integer\"}\n",
    "NUM_ROLLOUTS_PER_CASE = 5  # @param {type:\"integer\"}\n",
    "NUM_VALIDATION_CASES = 50  # @param {type:\"integer\"}\n",
    "VALIDATION_FREQUENCY = 5  # @param {type:\"integer\"}\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"finance_autocomplete_model\"  # @param {type:\"string\"}\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # @param {type:\"string\"}\n",
    "VAL_BENCHMARK_MODEL = \"gpt-4.1-nano\"  # @param {type:\"string\"}\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 5e-6  # @param {type:\"number\"}\n",
    "BETA = 0.0  # @param {type:\"number\"}\n",
    "\n",
    "# Reward configuration\n",
    "USE_JUDGE = OPENAI_API_KEY != \"\"  # Use LLM judge if API key provided\n",
    "EFFICIENCY_WEIGHT = 0.2  # @param {type:\"number\"}\n",
    "COMPLETION_WEIGHT = 0.3  # @param {type:\"number\"}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Training steps: {NUM_STEPS}\")\n",
    "print(f\"  Cases per step: {NUM_CASES_PER_STEP}\")\n",
    "print(f\"  Rollouts per case: {NUM_ROLLOUTS_PER_CASE}\")\n",
    "print(f\"  Total trajectories per step: {NUM_CASES_PER_STEP * NUM_ROLLOUTS_PER_CASE}\")\n",
    "print(f\"  Using LLM judge: {USE_JUDGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "import art\n",
    "\n",
    "# Import our finance modules\n",
    "from database import setup_database, get_tickers_with_data\n",
    "from synthetic import generate_cases, generate_training_data\n",
    "from environment import FinancialEnvironment\n",
    "from agent import AutocompleteAgent\n",
    "from rewards import calculate_reward\n",
    "from rollout import (\n",
    "    run_single_rollout,\n",
    "    conduct_rollouts,\n",
    "    run_validation,\n",
    "    generate_training_trajectories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the financial database\n",
    "await setup_database()  # Requires TIINGO_API_KEY\n",
    "\n",
    "# Verify data\n",
    "tickers = await get_tickers_with_data()\n",
    "print(f\"\\nDatabase contains data for {len(tickers)} tickers: {tickers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_STEPS = 100  # @param {type:\"integer\"}\n",
    "NUM_CASES_PER_STEP = 10  # @param {type:\"integer\"}\n",
    "NUM_ROLLOUTS_PER_CASE = 5  # @param {type:\"integer\"}\n",
    "NUM_VALIDATION_CASES = 50  # @param {type:\"integer\"}\n",
    "VALIDATION_FREQUENCY = 5  # @param {type:\"integer\"}\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"finance_autocomplete_model\"  # @param {type:\"string\"}\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # @param {type:\"string\"}\n",
    "VAL_BENCHMARK_MODEL = \"gpt-4.1-mini\"  # @param {type:\"string\"}\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 5e-6  # @param {type:\"number\"}\n",
    "BETA = 0.0  # @param {type:\"number\"}\n",
    "\n",
    "# Reward configuration\n",
    "USE_JUDGE = OPENAI_API_KEY != \"\"  # Use LLM judge if API key provided\n",
    "EFFICIENCY_WEIGHT = 0.2  # @param {type:\"number\"}\n",
    "COMPLETION_WEIGHT = 0.3  # @param {type:\"number\"}\n",
    "\n",
    "# Data configuration\n",
    "USE_TIINGO = TIINGO_API_KEY != \"\"  # Use real data if API key provided\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Training steps: {NUM_STEPS}\")\n",
    "print(f\"  Cases per step: {NUM_CASES_PER_STEP}\")\n",
    "print(f\"  Rollouts per case: {NUM_ROLLOUTS_PER_CASE}\")\n",
    "print(f\"  Total trajectories per step: {NUM_CASES_PER_STEP * NUM_ROLLOUTS_PER_CASE}\")\n",
    "print(f\"  Using LLM judge: {USE_JUDGE}\")\n",
    "print(f\"  Using Tiingo data: {USE_TIINGO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the financial database\n",
    "await setup_database(tiingo_api_key=TIINGO_API_KEY)\n",
    "\n",
    "# Verify data\n",
    "tickers = await get_tickers_with_data()\n",
    "print(f\"\\nDatabase contains data for {len(tickers)} tickers: {tickers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample test cases to verify everything works\n",
    "sample_cases = await generate_cases(5)\n",
    "print(\"Sample test cases:\")\n",
    "for i, case in enumerate(sample_cases, 1):\n",
    "    print(f\"\\n{i}. Input: {case['input']}\")\n",
    "    print(f\"   Expected: {case['ground_truth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainable model\n",
    "from art.local import LocalBackend\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=MODEL_NAME,\n",
    "    project=\"finance-autocomplete-rl\",\n",
    "    base_model=BASE_MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Create benchmark model for validation\n",
    "benchmark_model = art.Model(\n",
    "    name=VAL_BENCHMARK_MODEL,\n",
    "    project=\"finance-autocomplete-rl\",\n",
    "    inference_model_name=VAL_BENCHMARK_MODEL,\n",
    "    inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    inference_base_url=\"https://api.openai.com/v1\",\n",
    ")\n",
    "\n",
    "# Setup backend\n",
    "backend = LocalBackend(path=\"./.art\")\n",
    "\n",
    "# Register models\n",
    "await model.register(backend)\n",
    "await benchmark_model.register(backend)\n",
    "\n",
    "print(f\"Models registered:\")\n",
    "print(f\"  Training model: {MODEL_NAME}\")\n",
    "print(f\"  Benchmark model: {VAL_BENCHMARK_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Rollout\n",
    "\n",
    "Before training, let's test a single rollout to ensure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single rollout\n",
    "test_case = sample_cases[0]\n",
    "print(f\"Testing rollout with: {test_case['input']}\")\n",
    "print(f\"Expected: {test_case['ground_truth']}\")\n",
    "\n",
    "result = await run_single_rollout(\n",
    "    model=model,\n",
    "    test_case=test_case,\n",
    "    rollout_id=0,\n",
    "    step=0,\n",
    "    use_judge=USE_JUDGE\n",
    ")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(f\"\\nPrediction: {result['completion']}\")\n",
    "    print(f\"Reward: {result['reward_info']['total_reward']:.3f}\")\n",
    "    print(f\"  - Correctness: {result['reward_info']['correctness_score']:.3f}\")\n",
    "    print(f\"  - Efficiency: {result['reward_info']['efficiency_bonus']:.3f}\")\n",
    "    print(f\"  - Completion: {result['reward_info']['completion_penalty']:.3f}\")\n",
    "    print(f\"Tool calls: {result['episode_info']['tool_calls_count']}\")\n",
    "else:\n",
    "    print(f\"Error: {result.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for step in range(await model.get_step(), NUM_STEPS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Step {step}/{NUM_STEPS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate training cases for this step\n",
    "    train_cases = await generate_cases(NUM_CASES_PER_STEP)\n",
    "    \n",
    "    # Conduct rollouts\n",
    "    trajectory_groups = await conduct_rollouts(\n",
    "        model=model,\n",
    "        test_cases=train_cases,\n",
    "        num_rollouts_per_case=NUM_ROLLOUTS_PER_CASE,\n",
    "        step=step,\n",
    "        use_judge=USE_JUDGE\n",
    "    )\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    if trajectory_groups:\n",
    "        total_trajectories = sum(len(tg.trajectories) for tg in trajectory_groups)\n",
    "        avg_reward = sum(t.reward for tg in trajectory_groups for t in tg.trajectories) / total_trajectories\n",
    "        avg_correct = sum(t.metrics.get(\"is_correct\", 0) for tg in trajectory_groups for t in tg.trajectories) / total_trajectories\n",
    "        \n",
    "        print(f\"Training metrics:\")\n",
    "        print(f\"  Trajectories: {total_trajectories}\")\n",
    "        print(f\"  Avg reward: {avg_reward:.3f}\")\n",
    "        print(f\"  Accuracy: {avg_correct:.1%}\")\n",
    "    \n",
    "    # Run validation periodically\n",
    "    if step % VALIDATION_FREQUENCY == 0 and USE_JUDGE:\n",
    "        print(f\"\\nRunning validation...\")\n",
    "        val_trajectories = await run_validation(\n",
    "            my_model=model,\n",
    "            benchmark_model=benchmark_model,\n",
    "            num_validation_cases=NUM_VALIDATION_CASES,\n",
    "            step=step,\n",
    "            use_judge=USE_JUDGE\n",
    "        )\n",
    "        \n",
    "        if val_trajectories:\n",
    "            win_rate = sum(t.reward for t in val_trajectories) / len(val_trajectories)\n",
    "            print(f\"Validation win rate vs {VAL_BENCHMARK_MODEL}: {win_rate:.1%}\")\n",
    "            \n",
    "            # Log validation trajectories\n",
    "            await model.log(val_trajectories)\n",
    "    \n",
    "    # Train the model\n",
    "    if trajectory_groups:\n",
    "        await model.train(\n",
    "            trajectory_groups=trajectory_groups,\n",
    "            config=art.TrainConfig(\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                beta=BETA\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Clean up old checkpoints\n",
    "        await model.delete_checkpoints()\n",
    "        \n",
    "        print(f\"✓ Step {step} completed\")\n",
    "    else:\n",
    "        print(f\"⚠ No trajectories generated for step {step}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training completed!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Against Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final model against various baselines\n",
    "if USE_JUDGE:\n",
    "    print(\"Benchmarking against baseline models...\")\n",
    "    \n",
    "    # Define baseline models to compare\n",
    "    baseline_models = [\n",
    "        (\"gpt-4.1-nano\", \"gpt-4.1-nano\"),\n",
    "        (\"gpt-4.1-mini\", \"gpt-4.1-mini\"),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_id, model_name in baseline_models:\n",
    "        baseline_model = art.Model(\n",
    "            name=model_id,\n",
    "            project=\"finance-autocomplete-rl\",\n",
    "            inference_model_name=model_name,\n",
    "            inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            inference_base_url=\"https://api.openai.com/v1\",\n",
    "        )\n",
    "        \n",
    "        await baseline_model.register(backend)\n",
    "        \n",
    "        # Run evaluation\n",
    "        val_trajectories = await run_validation(\n",
    "            my_model=baseline_model,\n",
    "            benchmark_model=benchmark_model,\n",
    "            num_validation_cases=NUM_VALIDATION_CASES,\n",
    "            step=0,\n",
    "            use_judge=USE_JUDGE\n",
    "        )\n",
    "        \n",
    "        if val_trajectories:\n",
    "            win_rate = sum(t.reward for t in val_trajectories) / len(val_trajectories)\n",
    "            avg_reward = sum(t.metrics.get(\"my_reward\", 0) for t in val_trajectories) / len(val_trajectories)\n",
    "            results[model_id] = {\"win_rate\": win_rate, \"avg_reward\": avg_reward}\n",
    "            \n",
    "            # Log for comparison\n",
    "            await baseline_model.log(val_trajectories)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(f\"{'Model':<20} {'Win Rate':<15} {'Avg Reward':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for model_id, metrics in results.items():\n",
    "        print(f\"{model_id:<20} {metrics['win_rate']:.1%}{'':.<8} {metrics['avg_reward']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize training progress\n",
    "from art.utils.benchmarking.load_trajectories import load_trajectories\n",
    "from art.utils.benchmarking.charts import training_progress_chart\n",
    "from art.utils.benchmarking.types import BenchmarkModelKey\n",
    "\n",
    "# Load trajectories\n",
    "df = await load_trajectories(\n",
    "    project_name=\"finance-autocomplete-rl\",\n",
    "    models=[MODEL_NAME],\n",
    "    art_path=\"./.art\",\n",
    ")\n",
    "\n",
    "# Plot win rate over time\n",
    "if not df.empty and USE_JUDGE:\n",
    "    models_to_plot = [\n",
    "        BenchmarkModelKey(MODEL_NAME, MODEL_NAME, \"val\"),\n",
    "    ]\n",
    "    \n",
    "    for model_id, _ in baseline_models:\n",
    "        if model_id in results:\n",
    "            models_to_plot.append(BenchmarkModelKey(model_id, model_id.upper(), \"val\"))\n",
    "    \n",
    "    chart = training_progress_chart(\n",
    "        df,\n",
    "        \"win_rate\",\n",
    "        models=models_to_plot,\n",
    "        title=\"Win Rate vs Benchmark Over Time\",\n",
    "        y_label=\"Win Rate\",\n",
    "    )\n",
    "    \n",
    "    chart.savefig(\"finance_autocomplete_training.png\")\n",
    "    print(\"Training chart saved to finance_autocomplete_training.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final trained model on some examples\n",
    "print(\"Testing final model on sample inputs...\\n\")\n",
    "\n",
    "test_inputs = [\n",
    "    \"Apple's revenue in 2023 was \",\n",
    "    \"The gross margin for Microsoft in 2023Q4 was \",\n",
    "    \"Google's market cap in 2023Q4 was \",\n",
    "    \"The debt to equity ratio for Apple in 2023FY was \",\n",
    "    \"The CFO mentioned that \",\n",
    "]\n",
    "\n",
    "agent = AutocompleteAgent(model=model)\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    print(f\"Input: {input_text}\")\n",
    "    \n",
    "    completion, tool_calls, info = await agent.get_completion(input_text)\n",
    "    \n",
    "    print(f\"Completion: {completion}\")\n",
    "    print(f\"Tool calls: {info['tool_calls_count']}\")\n",
    "    \n",
    "    if tool_calls and info['tool_calls_count'] <= 10:\n",
    "        print(\"Tool sequence:\")\n",
    "        for tc in tool_calls[:10]:  # Show first 10 tools\n",
    "            args_str = \", \".join(f\"{k}={v}\" for k, v in tc.get('arguments', {}).items())\n",
    "            print(f\"  - {tc['tool']}({args_str})\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training configuration and results\n",
    "import json\n",
    "\n",
    "training_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"num_steps\": NUM_STEPS,\n",
    "    \"cases_per_step\": NUM_CASES_PER_STEP,\n",
    "    \"rollouts_per_case\": NUM_ROLLOUTS_PER_CASE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"beta\": BETA,\n",
    "    \"use_judge\": USE_JUDGE,\n",
    "    \"use_tiingo\": USE_TIINGO,\n",
    "    \"efficiency_weight\": EFFICIENCY_WEIGHT,\n",
    "    \"completion_weight\": COMPLETION_WEIGHT,\n",
    "}\n",
    "\n",
    "with open(\"training_info.json\", \"w\") as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"Training information saved to training_info.json\")\n",
    "print(f\"\\nModel checkpoint available at: ./.art/{MODEL_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
